Процесс - работа команды,приложения,приложения которое порождает другие процессы. У каждого процесса есть свой
идентификационный номер PID ,а у дочернего PPID - Parent process identification.
Процессы бывают:
1. Активен - Процесс в очереди на выполнение 
2.Спящие: т.е процесс не исполняемый в данный момент и не использующий память, либо процесс который передал управление дочернему.
«Зомби» (Z=Zombie) – это процесс, выполнение которого завершилось, но относящиеся к нему структуры ядра по каким-то причинам не освобождены. Одной из причин их появления в системе может быть следующая ситуация. Обычно освобождение структур ядра, относящихся к процессу, выполняет процесс-родитель после получения от потомка сигнала о завершении. Но бывают случаи, когда родительский процесс завершается раньше дочернего. Процессы, не имеющие родителя, называются "сиротами". "Сироты" автоматически усыновляются процессом init, который и принимает сигналы об их завершении. Если процесс-родитель или init по каким-то причинам не может принять сигнал о завершении дочернего процесса, то процесс-потомок превращается в "зомби" и получает статус Z. Процессы-зомби не занимают процессорного времени (т. е. их выполнение прекращается), но соответствующие им структуры ядра не освобождаются. В некотором смысле это «мертвые» процессы. Уничтожение таких процессов — одна из обязанностей системного администратора. Хочу отметить, что появление данных процессов говорит о том, что в системе что-то не в порядке, и скорее всего не в порядке с аппаратной частью, так что берем memtest и MHDD и тестим-тестим. Не исключен вариант и кривого кода программы.3. 
Виртуальная память – это адресное пространство процесса. 
Процесс работает не с физической памятью напрямую, а с виртуальной. 
Такая абстракция позволяет проще писать код приложений, не думать о том, 
что можно случайно обратиться не на те адреса памяти или адреса другого процесса. 
Это упрощает разработку приложений, а также позволяет превышать размер основной оперативной памяти за счёт 
описанных ниже механизмов. Виртуальная память состоит из основной памяти и swap-устройства. 
То есть объём виртуальной памяти может быть в принципе неограниченного размера.

       Виртуальная память - основная
                          |- swap
                          Т.к. в большинстве случаев, демоны в Linux простаивают и ожидают поступления каких-либо данных, соответственно, нужны относительно редко, так что держать их в памяти постоянно загруженными и расходовать на это ресурсы системы нерационально. Для организации работы демонов придуман демон inetd или его более защищенная модификация xinetd (eXtended InterNET Daemon или расширенный Интернет демон). В функции inetd (Xinetd) можно выделить:

установить ограничение на количество запускаемых серверов (служб, демонов)
наблюдение за соединениями на определенных портах и обработка входящих запросов
ограничение доступа к сервисам на основе ACL (списков контроля доступа)
Все процессы в системе, не важно Linux это или другая ОС, обмениваются между собой какой-либо информацией. Отсюда можно задать вопрос, а как же происходит межПРОЦЕССный обмен?

В ОС LINUX существует несколько видов можпроцессного обмена, а точнее сказать средств межпроцессного взаимодействия (Interprocess Communication - IPC), которые можно разбить на несколько уровней:

локальный (привязаны к процессору и возможны только в пределах компьютера);
-- каналы
pipe (они же конвейеры, так же неименованные каналы), о них я много рассказывал в прошлом посте, примером можно привести: команда1 | команда2. По сути, pipe использует stdin, stdout и stderr.
Именованные каналы (FIFO: First In First Out). Данный вид канала создаётся с помощью mknod или mkfifo, и два различных процесса могут обратиться к нему по имени. Пример работы с fifo:
в первом терминале (создаем именованный канал в виде файла pipe и из канала направляем данные с помощью конвейера в архиватор):

[root@proxy 1]# mkfifo pipe
[root@proxy 1]# ls -l
total 0
prw-r--r-- 1 root root 0 Nov  9 19:41 pipe
[root@proxy 1]# gzip -9 -c < pipe > out
во втором терминале (отправляем в именованный канал данные):

[root@proxy 1]# cat /path/to/file > pipe
в результате это приведет к сжатию передаваемых данных gzip-ом

-- сигналы
с терминала, нажатием специальных клавиш или комбинаций (например, нажатие Ctrl-C генерирует SIGINT, а Ctrl-Z SIGTSTP);
ядром системы:
при возникновении аппаратных исключений (недопустимых инструкций, нарушениях при обращении в память, системных сбоях и т. п.);
ошибочных системных вызовах;
для информирования о событиях ввода-вывода;
одним процессом другому (или самому себе), с помощью системного вызова kill(), в том числе:
из шелла, утилитой /bin/kill.
сигнал — это асинхронное уведомление процесса о каком-либо событии. Когда сигнал послан процессу, операционная система прерывает выполнение процесса. Если процесс установил собственный обработчик сигнала, операционная система запускает этот обработчик, передав ему информацию о сигнале. Если процесс не установил обработчик, то выполняется обработчик по умолчанию.
Все сигналы начинаются на «SIG…» и имеют числовые соответствия, определяемые в заголовочном файле signal.h. Числовые значения сигналов могут меняться от системы к системе, хотя основная их часть имеет в разных системах одни и те же значения. Утилита kill позволяет задавать сигнал как числом, так и символьным обозначением.
Сигналы можно послать следующими способами:

-- разделяемая память
Разделяемую память применяют для того, чтобы увеличить скорость прохождения данных между процессами. В обычной ситуации обмен информацией между процессами проходит через ядро. Техника разделяемой памяти позволяет осуществить обмен информацией не через ядро, а используя некоторую часть виртуального адресного пространства, куда помещаются и откуда считываются данные.

После создания разделяемого сегмента памяти любой из пользовательских процессов может подсоединить его к своему собственному виртуальному пространству и работать с ним, как с обычным сегментом памяти.

-- очереди сообщений
В общих чертах обмен сообщениями выглядит примерно так: один процесс помещает сообщение в очередь посредством неких системных вызовов, а любой другой процесс может прочитать его оттуда, при условии, что и процесс-источник сообщения и процесс-приемник сообщения используют один и тот же ключ для получения доступа к очереди.

удаленный;
-- удаленные вызовы процедур (Remote Procedure Calls - RPC)
RPC — разновидность технологий, которая позволяет компьютерным программам вызывать функции или процедуры в другом адресном пространстве (как правило, на удалённых компьютерах). Обычно, реализация RPC технологии включает в себя два компонента: сетевой протокол (чаще TCP и UDP, реже HTTP) для обмена в режиме клиент-сервер и язык сериализации объектов (или структур, для необъектных RPC).

-- сокеты Unix
Сокеты UNIX бывают 2х типов: локальные и сетевые. При использовании локального сокета, ему присваивается UNIX-адрес и просто будет создан специальный файл (файл сокета) по заданному пути, через который смогут сообщаться любые локальные процессы путём простого чтения/записи из него. Сокеты представляют собой виртуальный объект, который существует, пока на него ссылается хотя бы один из процессов. При использовании сетевого сокета, создается абстрактный объект привязанный к слушающему порту операционной системы и сетевому интерфейсу,  ему присваивается INET-адрес, который имеет адрес интерфейса и слушающего порта.

высокоуровневый
Обычно - пакеты программного обеспечения, которые реализуют промежуточный слой между системной платформой и приложением. Эти пакеты предназначены для переноса уже испытанных протоколов коммуникации приложения на более новую архитектуру. Примером можно привести: DIPC, MPI и др. (мне не знакомы, честно говоря)
Итак. Подведем маленький итог:

В Linux есть процессы,
каждый процесс может запускать подпроцессы (нити),
создание нового процесса создается клонированием исходного,
прородителем всех процессов в системе является процесс init, запускаемый ядром системы при загрузке.
процессы взаимодействуют между собой по средствам можпроцессного взаимодействия:
каналы
сигналы
сокеты
разделяемая память
каждый процесс обладает свойствами (читай: обладает следующим контекстом):
PID - идентификатор процесса
PPID - идентификатор процесса, породившего данный
UID и GID - идентификаторы прав процесса (соответствует UID и GID  пользователя, от которого запущен процесс)
приоритет процесса
состояние процесса (выполнение, сон и т.п.)
так же у процесса есть таблица открытых (используемых) файлов
Далее поговорим о том, как посмотреть состояние процессов в Linux и о том, как же ими управлять.

Управление процессами
Получение информации о процессе
Перед тем как управлять процессами, нужно научиться получать о процессах необходимую информацию. В Linux существует псевдофайловая система procfs, которая в большинстве дистрибутивов монтируется в общую ФС в каталог /proc. У данной файловой системы нет физического места размещения, нет блочного устройства, такое как жесткий диск. Вся информация, хранимая в данном каталоге находится в оперативной памяти компьютера, контролируется ядром ОС и она не предназначена для хранения файлов пользователя. О структуре данного каталога я написал в статье о файловой системе Linux. В этой файловой системе дано достаточно много информации, чтобы узнать о процессах и о системе в целом.

Но пользоваться данным каталогом очень не удобно, чтобы узнать о каком-либо процессе информацию, придется просмотреть кучу файлов и каталогов. Чтобы избавиться от ненужного труда, можно использовать существующие утилиты ps и top для просмотра информации о процессах.

Чтобы получить список всех процессов, достаточно ввести команду:

# ps aux

Прокомментируем некоторые интересные моменты. Можно заметить, что некоторые процессы указаны в квадратных скобках [ ] – это процессы, которые входят непосредственно в состав ядра и выполняют важные системные задачи, например, такие как управление буферным кэшем [pdflush] и организацией свопинга [kswapd]. С ними лучше не экспериментировать – ничего хорошего из этого не выйдет :). Остальная часть процессов относится к пользовательским.

Какую информацию можно получить по каждому процессу (комментарии к некоторым полям):

PID, PPID – идентификатор процесса и его родителя.
%CPU – доля процессорного времени, выделенная процессу.
%MEM – процент используемой оперативной памяти.
VSZ – виртуальный размер процесса.
TTY – управляющий терминал.
STAT– статус процесса:
R – выполняется;
S – спит;
Z – зомби;
< – Повышенный приоритет;
+ – Находится в интерактивном режиме.
START – время запуска.
TIME – время исполнения на процессоре.
Команда ps делает моментальный снимок процессов в текущий момент. В отличии от нее, команда top - динамически выводит состояние процессов и их активность в реальном режиме времени.

Пример вывода команды top:

14:32:49 up 35 days,  6:01,  4 users,  load average: 0.65, 0.51, 0.49
Tasks: 432 total,   1 running, 431 sleeping,   0 stopped,   0 zombie
CPU0  :  1.6%us,  3.6%sy,  0.0%ni, 85.3%id,  9.2%wa,  0.0%hi,  0.3%si,  0.0%st
CPU1  :  0.9%us,  1.9%sy,  0.0%ni, 96.9%id,  0.0%wa,  0.0%hi,  0.3%si,  0.0%st
Mem:   1033596K total,  1016644K used,    16952K free,    82928K buffers
Swap:  2096376K total,    12632K used,  2083744K free,   478220K cached

  PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND
 2788 root      15  -5     0    0    0 S  2.0  0.0 404:43.97 md3_raid1
 7961 root      20   0  5964 2528 1684 R  2.0  0.2   0:00.14 top
 6629 root      20   0  8612 2884 2096 S  0.7  0.3   0:00.96 dovecot-auth
   57 root      15  -5     0    0    0 S  0.3  0.0   4:36.10 kblockd/0
 8703 ulogd     20   0 17700 4216  656 S  0.3  0.4  87:23.98 ulogd
11336 ldap      20   0  394M  15M 8292 S  0.3  1.5   5:29.28 slapd
25757 ldap      20   0  394M  15M 8292 S  0.3  1.5   5:11.71 slapd
10991 root      20   0  2188 1004  588 S  0.3  0.1   4:23.33 dovecot
    1 root      20   0  1712  516  464 S  0.0  0.0   0:46.17 init
    2 root      15  -5     0    0    0 S  0.0  0.0   0:00.00 kthreadd
    3 root      RT  -5     0    0    0 S  0.0  0.0   0:05.92 migration/0
.....
 2960 root      16  -4  1980  520  392 S  0.0  0.1   0:10.04 udevd
 2993 dovecot   20   0  4412 1800 1476 S  0.0  0.2   0:00.00 pop3-login
 2994 dovecot   20   0  4412 1800 1476 S  0.0  0.2   0:00.02 pop3-login
В верхней части вывода отображается астрономическое время, время, прошедшее с момента запуска системы, число пользователей в системе, число запущенных процессов и число процессов, находящихся в разных состояниях, данные об использовании ЦПУ, памяти и свопа. А далее идет таблица, характеризующая отдельные процессы. Число строк, отображаемых в этой таблице, определяется размером окна: сколько строк помещается, столько и выводится.

Содержимое окна обновляется каждые 5 секунд. Список процессов может быть отсортирован по используемому времени ЦПУ (по умолчанию), по использованию памяти, по PID, по времени исполнения. Переключать режимы отображения можно с помощью следующих клавиатурных команд:

<Shift>+<N> — сортировка по PID;
<Shift>+<A> — сортировать процессы по возрасту;
<Shift>+<P> — сортировать процессы по использованию ЦПУ;
<Shift>+<M> — сортировать процессы по использованию памяти;
<Shift>+<T> — сортировка по времени выполнения.
С помощью команды <K> можно завершить некоторый процесс (его PID будет запрошен), а с помощью команды <R> можно переопределить значение nice для некоторого процесса.

Полезную информацию, так же, позволяет получить программа lsof, которая выдает список всех файлов, используемых сейчас процессами, включая каталоги, занятые потому, что какой-либо процесс использует их в качестве текущего или корневого; разделяемые библиотеки, загруженные в память; и т. д.

Итак, теперь об управлении процессами.

Управление процессами в Linux
Каждому процессу при запуске устанавливается определенный приоритет, который имеет значение от -20 до +20, где +20 - самый низкий. Приоритет нового процесса равен приоритету процесса-родителя. Для изменения приоритета запускаемой программы существует утилита nice. Пример ее использования:

[root@proxy 1]# nice [- adnice] command [args]
где adnice — значение (от –20 до +19), добавляемое к значению nice процесса-родителя. Отрицательные значения может устанавливать только суперпользователь. Если опция adnice не задана, то по умолчанию для процесса-потомка устанавливается значение nice, увеличенное на 10 по сравнению со значением nice родительского процесса.

Команда renice служит для изменения значения nice для уже выполняющихся процессов. Суперпользователь может изменить приоритет любого процесса в системе. Другие пользователи могут изменять значение приоритета только для тех процессов, для которых данный пользователь является владельцем. При этом обычный пользователь может только уменьшить значение приоритета. Поэтому процессы с низким приоритетом не могут породить "высокоприоритетных детей".

Как я уже писал, одним из средств управления процессами являются сигналы. Некоторые сигналы можно сгенерировать с помощью определенных комбинаций клавиш, но такие комбинации существуют не для всех сигналов. Зато имеется команда kill, которая позволяет послать заданному процессу (указав его PID) любой сигнал:

[root@proxy 1]# kill [-SIG] PID [PID..]
где SIG — это номер сигнала или наименование сигнала, причем если указание сигнала опущено, то посылается сигнал 15 (SIGTERM — программное завершение процесса). Часто используется сигнал 9 (KILL), с помощью которого суперпользователь может завершить любой процесс. Но сигнал этот очень "грубый", если можно так выразиться, потому что он просто «убивает» процесс, не давая ему времени на корректное сохранение всех обработанных данных. Поэтому в большинстве случаев рекомендуется использовать сигналы TERM или QUIT, которые завершают процесс более "мягко". Если процессу необходимо как-то по-особенному реагировать на сигнал, он может зарегистрировать обработчик, а если обработчика нет, за него отреагирует система.

Два сигнала – 9 ( KILL ) и 19 ( STOP ) – всегда обрабатывает система. Первый из них нужен для того, чтобы убить процесс наверняка (отсюда и название). Сигнал STOP приостанавливает процесс: в таком состояниипроцесс не удаляется из таблицы процессов, но и не выполняется до тех пор, пока не получит сигнал 18 ( CONT) – после чего продолжит работу. В Linux сигнал STOP можно передать активному процессу с помощью управляющего символа " ^Z ".

Обычные пользователи могут посылать сигналы только тем процессам, для которых они являются владельцами. Если в команде kill воспользоваться идентификатором процесса (PID), равным -1, то указанный в команде сигнал будет послан всем принадлежащим данному пользователю процессам. Суперпользователь root может посылать сигналы любым процессам. Когда суперпользователь посылает сигнал идентификатору -1, он рассылается всем процессам, за исключением системных. Если этим сигналом будет SIGKILL, то у простых пользователей будут потеряны все открытые ими, но не сохраненные файлы данных.

При обычном запуске процесс работает на переднем плане. то есть процесс "привязывается" к терминалу, с которого он запущен, воспринимая ввод с этого терминала и осуществляя на него вывод. Но можно запустить процесс в фоновом режиме, когда он не связан с терминалом, для чего в конце командной строки запуска программы добавляют символ &.

В оболочке bash имеются две встроенные команды, которые служат для перевода процессов на передний план или возврата их в фоновый режим. Команда fg переводит указанный в аргументе процесс на передний план, а команда bg — переводит процесс в фоновый режим. Одной командой bg можно перевести в фоновый режим сразу несколько процессов, а вот возвращать их на передний план необходимо по одному. Аргументами команд fg и bg могут являться только номера заданий, запущенных из текущего экземпляра shell. Возможные значения заданий можно увидеть, выполнив команду jobs.

При завершении сессии оболочка посылает всем порожденным ею процессам сигнал "отбой", по которому порожденные ею процессы могут завершиться, что не всегда желательно. Если вы хотите запустить в фоновом режиме программу, которая должна выполняться и после вашего выхода из оболочки, то ее нужно запускать с помощью утилиты nohup:

[root@proxy 1]# nohup команда &
Запущенный таким образом процесс будет игнорировать посылаемые ему сигналы (не игнорируются только сигналы SIGHUP и SIGQUIT). Хочу так же выделить команду pstree, которая показывает дерево процессов. Очень наглядно, кстати.

Ну вот. как-то так. Буду рад любым комментариям и дополнениям.

Upd 2011.11.19: небольшой рестайлинг и дополнение информации о сигналах.
 
 Для управления виртуальной памятью в системе присутствует параметр overcommit. Он следит за тем, чтобы мы не переиспользовали размер памяти. Управляется через sysctl и может быть в следующих трех значениях:
0 – значение по умолчанию. В этом случае используется эвристика, которая следит за тем, чтобы мы не смогли выделить виртуальной памяти в процессе намного больше, чем есть в системе;
1 – говорит о том, что мы никак не следим за объёмом выделяемой памяти. Это полезно, например, в программах для вычислений, которые выделяют большие массивы данных и работают с ними особым способом;
2 – параметр, который позволяет строго ограничивать объем виртуальной памяти процесса.

Посмотреть, сколько у нас закоммичено памяти, сколько используется и сколько мы можем ещё выделить, можно в строках CommitLimit и Commited_AS из файла /proc/meminfo.
 Память процесса

Процесс состоит из следующих сегментов. У нас есть stack, который растет вниз; у него есть лимит дальше котрого он расти не может. 



Затем идет регион mmap: там находятся все отображенные на память файлы процесса, которые мы открыли или создали через системный вызов mmap(). Далее идет большое пространство невыделенной виртуальной памяти, которую мы можем использовать. Снизу вверх растет heap – это область анонимной памяти. Внизу идут области бинарника, который мы запускаем.

Если мы говорим о памяти внутри процесса, то работать со страницами тоже неудобно: как правило, выделение памяти внутри процесса происходит блоками. Очень редко требуется выделить одну­-две странички, обычно нужно выделить сразу какой-­то промежуток страниц. Поэтому в Linux существует такое понятие, как область памяти (virtual memory area, VMA), которая описывает какое-­то пространство адресов внутри виртуального адресного пространства этого процесса. На каждую такую VMA есть свои права (чтения, записи, исполнения) и области видимости: она может быть приватная или общая (которая «шарится (share)» с другими процессами в системе).

Выделение памяти

Выделение памяти можно поделить на четыре случая: есть выделение приватной памяти и памяти, которой можем с кем-­то поделиться (share); двумя другими категорями являются разделение на анонимную память и ту, у которая связана с файлом на диске. Самые частые функции выделения памяти – это malloc и free. Если мы говорим о glibc malloc(), то он выделяет анонимную память таким интересным способом: использует heap для аллокации маленьких объемов (менее 128 KБ) и mmap() для больших объемов. Такое выделение необходимо для того, чтобы память расходовалась оптимальнее и её можно было запросто отдавать в систему.  Если в heap не хватает памяти для выделения, вызывается системный вызов brk(), который расширяет границы heap. Системный вызов mmap() занимается тем, что отображает содержимое файла на адресное пространство. munmap() в свою очередь освобождает отображение. У mmap() есть флаги, которые регулируют видимость изменений и уровень доступа. 



На самом деле, Linux не выделяет всю запрошенную память сразу. Процесс выделения памяти — Demand Paging — начинается с того, что мы запрашиваем у ядра системы страничку памяти, и она попадает в область Only Allocated. Ядро отвечает процессу: вот твоя страница памяти, ты можешь её использовать. И больше ничего происходит. Никакой физической аллокации не происходит. А произойдет она только в том случае, если мы попробуем в эту страницу произвести запись. В этот момент пойдёт обращение в Page Table – эта структура транслирует виртуальные адреса процесса в физические адреса оперативной памяти. При этом будут задействованы также два блока: MMU и TLB, как видно из рисунка. Они позволяют ускорять выделение и служат для трансляции виртуальных адресов в физические.



После того, как мы понимаем, что этой странице в Page Table ничего не соответствует, то есть нет связи с физической памятью, мы получаем Page Fault – в данном случае минорный (minor), так как отсутствует обращение в диск. После этого процесса система может производить запись в выделенную страницу памяти. Для процесса все это происходит прозрачно. А мы можем наблюдать увеличение счетчика минорных Page Fault для процесса на одну единицу. Также бывает мажорный Page Fault – в случае, когда происходит обращение в диск за содержимым страницы (в случае mmpa()).

Один из трюков в работе с памятью в Linux – Copy On Write – позволяет делать очень быстрые порождения процессов (fork).

Работа с файлами и с памятью

Подсистема памяти и подсистема работы с файлами тесно связаны. Так как работа с диском напрямую очень медленна, ядро использует в качестве прослойки оперативную память.

malloc() использует больше памяти: происходит копирование в user space. Также потребляется больше CPU, и мы получаем больше переключений контекста, чем если бы мы работали с файлом через mmap().

Какие выводы можно сделать? Мы можем работать с файлами, как с памятью. У нас есть lazy lоading, то есть мы можем замапить очень­-очень большой файл, и он будет подгружаться в память процесса через Page Cache только по мере надобности. Всё также происходит быстрее, потому что мы используем меньше системных вызовов и, в конце концов, это экономит память. Ещё стоит отметить, что при завершении программы память никуда не девается и остается в Page Cache.

В начале было сказано, что вся запись и чтение идут через Page Cache, но иногда по какой­-то причине, есть необходимость в отходе от такого поведения. Некоторые программные продукты работают таким способом, например MySQL с InnoDB.

Подсказать ядру, что в ближайшее время мы не будем работать с этим файлом, и заставить выгрузить страницы файла из Page Cache можно с помощью специальных системных вызовов:
posix_fadvide();
madvise();
mincore().

Утилита vmtouch также может вынести страницы файла из Page Cache – ключ “­e”.

Readahead

Поговорим про Readahead. Если читать файлы с диска через Page Cache каждый раз постранично, то у нас будет достаточно много Page Fault и мы будем часто ходить на диск за данными. Поэтому мы можем управлять размером Readahead: если мы прочитали первую и вторую страничку, то ядро понимает, что, скорее всего, нам нужна и третья. И так как ходить на диск дорого, мы можем прочитать немного больше заранее, загрузив файл наперёд в Page Cache и отвечать в будущем из него. Таким образом происходит замена будущих тяжёлых мажорных (major) Page Faults на минорные (minor) page fault.

Итак мы выдали всем память, все процессы довольны, и внезапно память у нас закончилась. Теперь нам нужно ее как­-то освобождать. Процесс поиска и выделения свободной памяти в ядре называется Page Reclaiming. В памяти могут находится страницы памяти, которые нельзя забирать, – залокированные страницы (locked). Помимо них есть ещё четыре категории страниц. Cтраницы ядра, которые выгружать не стоит, потому что это затормозит всю работу системы; cтраницы Swappable – это такие страницы анонимной памяти, которые никуда, кроме как в swap устройство выгрузить нельзя; Syncable Pages – те, которые могут быть синхронизированы с диском, а в случае открытого файла только на чтение – такие страницы можно с лёгкостью выбросить из памяти; и Discardable Pages – это те страницы, от которых можно просто отказаться.

Источники пополнения Free List

Если говорить упрощённо, то у ядра есть один большой Free List (на самом деле, это не так), в котором хранятся страницы памяти, которые можно выдавать процессам. Ядро пытается поддерживать размер этого списка в каком­-то не нулевом состоянии, чтобы быстро выдавать память процессам. Пополняется этот список за счёт четырех источников: Page Cache, Swap, Kernel Memory и OOM Killer.

Мы должны различать участки памяти на горячую и холодную и как­-то пополнять за счет них наши Free Lists. Page Cache устроен по принципу LRU/2 ­очереди. Есть активный список страниц (Active List) и инактивный список (Inactive List) страничек, между которыми есть какая­-то связь. В Free List прилетают запросы на выделение памяти (allocation). Система отдаёт страницы из головы этого списка, а в хвост списка попадают страницы из хвоста инактивного (inactive) списка. Новые страницы, когда мы читаем файл через Page Cache, всегда попадают в голову и проходят до конца инактивного списка, если в эти страницы не было еще хотя бы одного обращения. Если такое обращение было в любом месте инактивного списка, то страницы попадают сразу в голову активного списка и начинают двигаться в сторону его хвоста. Если же в этот момент опять к ним происходит обращение, то страницы вновь пробиваются в верх списка. Таким образом система пытается сбалансировать списки: самые горячие данные всегда находятся в Page Cache в активном списке, и Free List никогда не пополняется за их счет.

Также тут стоит отметить интересное поведение: страницы, за счет которых пополняется Free List, которые в свою очередь прилетают из инактивного списка, но до сих пор не отданные для аллокации, могут быть возвращены обратно в инактивный списка (в данном случае в голову инактивного списка).

Итого у нас получается пять таких листов: Active Anon, Inactive Anon, Active File, Inactive File, Unevictable. Такие списки создаются для каждой NUMA ноды и для каждой Memory Zone.

Несколько слов о cgroups

С помощью cgroups мы можем ограничивать несколько процессов по любым параметрам. В данном случае нам интересна память: можем ограничить память без swap, а можем память и swap. На каждую группу мы можем прикрутить свой Out Of Memory Killer. При помощи cgroups можно удобно получать статистику использования памяти для процесса или группы процессов в разрезе аннонимной и не анонимной памяти, использовании Page Caсhe и прочее (/sys/fs/cgroup/memory/memory.stat). При использовании cgroups с ограничением памяти Page Reclaiming бывает двух видов: 
Global Reclaiming, когда мы ищем память для всей системы – пополняем Free Lists системы;
Target Reclaiming, когда мы освобождаем память в одной из cgroup – в случае нехватки памяти в ней.
 
 
 
 
 
Page Cache

Через Page Cache в Linux по умолчанию идут все операции чтения и записи. Он динамического размера, то есть именно он съест всю вашу память, если она свободна. Как гласит старая шутка, если вам нужна свободная память в сервере, просто вытащите ее из сервера.  Page Cache делит все файлы, которые мы читаем, на страницы (страница, как мы сказали, – 4 KБ).  Посмотреть, есть ли в Page Cache какие-­то страницы какого-­то конкретного файла, можно с помощью системного вызова mincore(). Или с помощью утилиты vmtouch, которая написана с использованием этого системного вызова.

Как же происходит запись? Любая запись происходит на диск не сразу, а в Page Cache, и делается это практически моментально. Тут можно увидеть интересную «аномалию»: запись на диск идет намного быстрее, чем чтение. Дело в том, что при чтении (если данной странички файла в Page Cache нет) мы пойдем в диск и будем синхронно ждать ответа, а запись в свою очередь пройдет моментально в кеш.

Минусом такого поведения является то, что на самом деле данные никуда не записались, — они просто находятся в памяти, и когда-­то их нужно будет сбросить на диск. У каждой странички при записи проставляется флажок (он называется dirty). Такая «грязная» страничка появляется в Page Cache. Если накапливается много таких страничек, система понимает, что пора их сбросить на диск, а то можно их потерять (если внезапно пропадет питание, наши данные тоже пропадут). 
 
 
 
 
 
 
 Как видно из слайда, у нас есть два процессора, которые общаются между собой по какому-­то каналу, и у каждого из них есть свои шины, через которые они общаются со своими банками памяти. Если мы посмотрим на картинку, то задержка от CPU 1 к RAM 1 в NUMA­-ноде будет в два раза меньше, чем от CPU 1 на RAM 2. Получить эти данные и прочую информацию мы можем, используя команду numactl ­­hardware.

Мы видим, что сервер имеет две ноды и информацию по ним (сколько в каждой ноде свободной физической памяти).  Память выделяется на каждой ноде отдельно. Поэтому можно потребить всю свободную память на одной ноде, а другую — недогрузить. Чтобы такого не было (это свойственно базам данных), можно запускать процесс с командой numactl ­­interleave=all. Это позволяет распределять выделение памяти между двумя нодам равномерно. В противном случае ядро выбирает ноду, на которой был запланирован запуск этого процесса (CPU scheduling) и всегда пытается выделить память на ней.

Также память в системе поделена на Memory Zones. Каждая NUMA-­нода делится на какое­-то количество таких зон. Они служат для поддержки специального железа, которое не может общаться по всему диапазону адресов. К примеру, ZONE_DMA – это 16 MБ первых адресов, ZONE_DMA32 – это 4 ГБ. Смотрим на зоны памяти и их состояние через файл /proc/zoneinfo.
 
 
 
                          
 Часто при дефолтном значении системных параметров ядра vm.overcommit_ratio=50 и vm.overcommit_memory=0 java или Redis получает ошибку «Cannot allocate memory».
Для избежания подобных проблем нужно выставить следующие параметры:

Установить значение vm.overcommit_ratio=100 вместо умолчальных 50.

echo 100 > /proc/sys/vm/overcommit_ratio
или
sysctl –w vm.overcommit_ratio=100
Внесите в файл /etc/sysctl.conf следующую строку: vm.overcommit_ratio = 100, что бы изменения сохранились после перезагрузки:

echo 'vm.overcommit_ratio = 100' >> /etc/sysctl.conf
Установить значение vm.overcommit_memory=2 вместо умолчального 0.

echo 2 > /proc/sys/vm/overcommit_memory
или
sysctl –w vm.overcommit_memory=2
Внесите в файл /etc/sysctl.conf следующую строку: vm.overcommit_memory = 2, что бы изменения сохранились после перезагрузки:

echo 'vm.overcommit_memory = 2' >> /etc/sysctl.conf
В ядре есть два основных параметра, отвечающих за overcommit памяти:

vm.overcommit_memory — отвечает за стратегию overcommit
vm.overcommit_ratio — отвечает за уровень (в процентах) overcommit-а
Стратегии есть такие (см. файл с исходниками ядра mm/mmap.c):

0 — OVERCOMMIT_GUESS — эвристический подход к распределению памяти. В нем выделяется столько памяти, сколько хочет процесс. Но в swap/res попадает только те страницы, которые используются этим процессом.
1 — OVERCOMMIT_ALWAYS — overcommit памяти есть всегда. Использовать лучше с совсем кривыми приложениями и быть готовым при этому ко всему.
2 — OVERCOMMIT_NEVER — без overcommit. В этом случае допустимый объем пространства памяти будет swap+ram*overcommit_ratio/100.
По умолчанию используется стратегия OVERCOMMIT_GUESS, а vm.overcommit_ratio находится в значение 50% и используется только в случае OVERCOMMIT_NEVER. Система резервирует около 3% памяти для процессов пользователя root.

Есть еще oom_killer, но с дефолтными настройками он очень часто прибивает не то, что нужно (например, sshd). Чтобы он работал более адекватно, нужно настраивать /proc//oom_adj, а это более трудоёмко, чем просто поменять пару параметров в sysctl.conf.
